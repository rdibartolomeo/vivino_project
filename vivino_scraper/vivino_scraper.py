import os
import requests
import json
import csv
import time
import pandas as pd
import numpy as np
import proxy_generator


class NapaScraper:

	"""
	A class used to scrap wine data from Vivino. The API used to make these
	requests is sorted by popularity, filtered by the Napa Valley region and
	for wines at or below $500.

	Attributes
	----------
	__api_url : str
		The url for the vivino api, with {} replacing the page number in the url
	__headers : dict
		Dictionary containing the headers to pass into the request to the api
	__elite_proxies : class
		Instantiates the EliteProxy class
	__random_proxy : str
		Random proxy generated by __elite_proxies
	__scraped_data : list
		List to store scraped vivino data

	Methods
	-------
	api_request(self, page_num):
		Makes a request to the vivino api at the page number passed
		and returns the request response

	"""

	def __init__(self):

		self.__api_url = 'https://www.vivino.com/api/explore/explore?country_code=US&currency_code=USD&grape_filter=varietal&min_rating=1&order_by=ratings_count&order=desc&page={}&price_range_max=500&price_range_min=0&region_ids[]=25&region_ids[]=24'
		self.__headers = headers = {
			'Accept-Encoding':'gzip, deflate, br',
			'Accept-Language':'en-US,en;q=0.9',
			'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_0) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',
		}
		self.__elite_proxies = proxy_generator.EliteProxy()
		self.__random_proxy = self.__elite_proxies.get_proxy()
		self.__scraped_data = []


	def __update_proxy(self):
		self.__random_proxy = self.__elite_proxies.get_proxy()


	def __api_request(self, page_num):

		# format the api url with the page number passed
		api_url = self.__api_url.format(page_num)
		headers = self.__headers

		# Make 200 connections attempts to the api
		for connection_attempt in range(200):
			try:
				# get a random proxy
				random_proxy = self.__random_proxy
				proxy = {'https': random_proxy, 'http': random_proxy}
				# make request to the api with the proxy with 10 second timeout
				response = requests.get(api_url,headers=headers, proxies=proxy, timeout=10)
			except:
				# if the proxy connection fails
				print('failed proxy # {}'.format(self.__random_proxy))
				self.__update_proxy()
			else:
				# if the proxy connection succeeds
				break
			# make randomized pause between attempts
			time.sleep(int(np.random.random()*5))

		# parse the json response to the request
		try:
			results = response.json()['explore_vintage']['matches']
		except:
			print('Unable to parse results for page {}'.format(page_num))
		else:
			print('successful connection on proxy # {}'.format(random_proxy))

		# return parsed json
		return results


	def __extract_all_grapes(self, wine, row):

		# locate dict of grapes
		grapes = wine['wine']['style']['grapes']
		n = int(len(grapes))-1

		# iterate through list of grapes
		for n in range(n+1):
			# format column name
			col_name = 'grape_' + grapes[n]['name']
			# assign column value for passed row
			row[col_name] = 1


	def __extract_all_flavors(self, wine, row):

		# locate dict of wine flavors
		flavors = wine['wine']['taste']['flavor']
		n = int(len(flavors))-1

		# instantiate variable for total flavor votes
		total_flavor_cnt = 0

		# calculate the total votes cast on wine flavors
		for n in range(n+1):
			total_flavor_cnt += flavors[n]['stats']['count']

		# calculate the percentage of total votes for each each flavor
		# as a flavor score
		for n in range(n+1):
			col_name = 'flavor_score_' + flavors[n]['group']
			row[col_name] = flavors[n]['stats']['count'] / total_flavor_cnt


	def __scrape_page(self, request_result):

		# for each wine matched
		for match_num in range(len(request_result)):

			# instantiate row dictionary
			row = {}
			# parse request result for specific match number
			wine = request_result[match_num]['vintage']

			# all wines have the following data points
			row['wine_name'] = wine['name']
			row['winery_name'] = wine['wine']['winery']['name']
			row['vintage'] = wine['year']
			row['ratings_count'] = wine['statistics']['ratings_count']
			row['ratings_average'] = wine['statistics']['ratings_average']
			row['wine_type'] = wine['wine']['name']
			row['natural_wine'] = wine['wine']['is_natural']


			# some wines do not have all of these data points listed below
			# exception handling is used to accomodate missing data
			try:
				row['price'] = request_result[match_num]['price']['amount']
			except:
				pass

			try:
				row['taste_acidity'] = wine['wine']['taste']['structure']['acidity']
				row['taste_fizziness'] = wine['wine']['taste']['structure']['fizziness']
				row['taste_intensity'] = wine['wine']['taste']['structure']['intensity']
				row['taste_sweetness'] = wine['wine']['taste']['structure']['sweetness']
				row['taste_tannin'] = wine['wine']['taste']['structure']['tannin']
			except:
				pass

			try:
				row['wine_style'] = wine['wine']['style']['name']
			except:
				pass

			try:
				self.__extract_all_grapes(wine, row)
			except:
				pass

			try:
				self.__extract_all_flavors(wine, row)
			except:
				pass

			# scraped row is appended to __scraped_data list
			self.__scraped_data.append(row)


	def scrape_data(self, page_start, page_end):

		for page_num in range(page_start,page_end + 1):

			try:
				# request json result for passed page number
				result = self.__api_request(page_num)
				# scrape the json result and append to __scraped_data list
				self.__scrape_page(result)
			except:
				# move on if page cannot be scraped
				print('Moving on to page {}...'.format(page_num+1))
				continue
			else:
				# print progress report
				print('Page {} / {} scraped üêç'.format(page_num, page_end))

		# determine all unique features scraped
		features = set().union(*(d.keys() for d in self.__scraped_data))

		# format file name
		file_name = "./wine_data/wine_data_page_{}_to_{}.csv".format(page_start, page_end)
		print('CSV saved in {}'.format(file_name))

		# check if directory exists, and make one if it does not
		if not os.path.isdir("./wine_data/"):
			os.mkdir("./wine_data/")

		# write scraped data to csv
		with open(file_name, 'w', newline='', encoding='utf8')  as csv_file:
			dict_writer = csv.DictWriter(csv_file, features)
			dict_writer.writeheader()
			dict_writer.writerows(self.__scraped_data)
